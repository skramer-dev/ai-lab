{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "from scipy.special import softmax\n",
    "import pandas as pd\n",
    "from IPython.display import Image\n",
    "from transformers import BertTokenizer\n",
    "from scipy.special import softmax\n",
    "from IPython.display import Image\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "from openprompt import PromptDataLoader, PromptForClassification\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, ManualTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(model, encoding, n):\n",
    "    outputs = model(**encoding)\n",
    "    predictions = outputs.logits.detach().numpy()[0]\n",
    "    predictions = [(idx, single_output) for idx, single_output in enumerate((softmax(predictions)*100))]\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[:n]\n",
    "\n",
    "def predict(model, inference_text, tokenizer,n):\n",
    "    inference_input = InputExample(text_a = inference_text)\n",
    "    inference_dataloader = PromptDataLoader(dataset=[inference_input], template=promptTemplate, tokenizer=tokenizer,\n",
    "        tokenizer_wrapper_class=WrapperClass, max_seq_length=250, decoder_max_length=3,\n",
    "        batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "        truncate_method=\"head\")\n",
    "\n",
    "    for index, inputs in enumerate(inference_dataloader):\n",
    "        logits = model(inputs)\n",
    "    predictions = [(idx, single_output) for idx, single_output in enumerate((softmax(logits.detach().numpy().tolist()[0])*100))]\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return predictions[:n]\n",
    "\n",
    "def create_input_text_list(input_text):\n",
    "    input_text_list = [[]]\n",
    "    line_input_length = int(len(input_text.split())/3)\n",
    "    max_length = 10\n",
    "    line_max = max_length if line_input_length < max_length else line_input_length if len(input_text) > max_length * 3 else max_length\n",
    "    print(line_max)\n",
    "    if len(input_text) > line_max:\n",
    "        word_list = input_text.split()\n",
    "        word_count = 0\n",
    "        word_idx = 0\n",
    "        for word in word_list:\n",
    "            if word_count < line_max:\n",
    "                input_text_list[word_idx].append(word)\n",
    "                word_count += 1\n",
    "            else:\n",
    "                word_count = 0\n",
    "                word_idx += 1\n",
    "                input_text_list.append([])\n",
    "        input_text_list = [\" \".join(text_list) for text_list in input_text_list]\n",
    "    else:\n",
    "        input_text_list.append(input_text)\n",
    "    return input_text_list\n",
    "\n",
    "def create_pretty_string(model_names, input_text, model_labels, top_n):\n",
    "    output_string = \"\"\n",
    "    border = \"  |  \"\n",
    "    input_title = \"Input\"\n",
    "    input_text_list = create_input_text_list(input_text)\n",
    "    output_string += \"######\\n\"\n",
    "    first_column_length = len(max(input_text_list + [input_title], key=len))\n",
    "    output_string += input_title\n",
    "    output_string += \"\".ljust(first_column_length - len(input_title), \" \")\n",
    "    for column_idx, name in enumerate(model_names):\n",
    "        output_string += border\n",
    "        output_string += name\n",
    "        column_length = len(max(model_labels[column_idx] + [name], key=len)) - len(name)\n",
    "        output_string += \"\".ljust(column_length, \" \")\n",
    "    output_string += border\n",
    "    output_string += \"\\n\"\n",
    "    output_string += (u'\\u2500'*(len(output_string))) + \"\\n\"\n",
    "    for row_idx in range(top_n):\n",
    "        if row_idx < len(input_text_list):\n",
    "            output_string += input_text_list[row_idx]\n",
    "            row_length = first_column_length - len(input_text_list[row_idx])\n",
    "            output_string += \"\".ljust(row_length, \" \")\n",
    "        else:\n",
    "            output_string += \"\".ljust(first_column_length, \" \")\n",
    "        output_string += border\n",
    "        for model_idx, model_output in enumerate(model_labels):\n",
    "            max_length_column = len(max(model_output + [model_names[model_idx]], key=len))\n",
    "            whitespace_length = max_length_column - len(model_output[row_idx])\n",
    "            output_string += model_output[row_idx]\n",
    "            output_string += \"\".ljust(whitespace_length, \" \")\n",
    "            output_string += border\n",
    "        output_string += \"\\n\"\n",
    "    output_string += \"######\\n\"\n",
    "    return output_string\n",
    "\n",
    "def pretty_inference(model_list, model_names, input_list, tokenizer_list, top_n):\n",
    "    output = \"\"\n",
    "    for input_text in input_list:\n",
    "        model_labels = [[] for _ in range(len(model_list))]\n",
    "        for idx, model in enumerate(model_list):\n",
    "            tokenizer = tokenizer_list[idx]\n",
    "            if \"prompting\" in model_names[idx]:\n",
    "                predictions = predict(model, input_text, tokenizer, top_n)\n",
    "            else:\n",
    "                encoding = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "                predictions = forward(model, encoding, top_n)\n",
    "            for prediction in predictions:\n",
    "                pk_name = mappings.loc[mappings[\"index\"]==prediction[0]][\"name\"].values[0]\n",
    "                model_labels[idx].append(f\"{pk_name}:{prediction[1]:.2f}%\")\n",
    "        output += create_pretty_string(model_names, input_text, model_labels, top_n)\n",
    "        output += \"\\n\"\n",
    "    clear_output()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using custom data configuration dataset-294e9b13f49dafc6\n",
      "Found cached dataset csv (C:/Users/fst/.cache/huggingface/datasets/csv/dataset-294e9b13f49dafc6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d504d06935bf4532b348ea61d0f6e392",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dependencies\n",
    "plm, prompt_tokenizer, model_config, WrapperClass = load_plm(\"gpt2\",\"gpt2\")\n",
    "mappings = pd.read_csv('data/pokemon_mapping.csv')\n",
    "name_to_label_dict = mappings[[\"name\",\"index\"]].set_index('index').to_dict()[\"name\"]\n",
    "pokemon_descriptions = load_dataset('data/dataset/', delimiter=';')\n",
    "NUM_CLASSES = np.unique(pokemon_descriptions['train']['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "promptTemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} the pokemon is {\"mask\"}',\n",
    "    tokenizer = prompt_tokenizer,\n",
    ")\n",
    "\n",
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = NUM_CLASSES,\n",
    "    label_words = name_to_label_dict,\n",
    "    tokenizer = prompt_tokenizer,\n",
    ")\n",
    "\n",
    "promptLoadedModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer,\n",
    "    freeze_plm= True\n",
    ")\n",
    "\n",
    "promptLoadedModel.load_state_dict(state_dict=torch.load(\"prompting/checkp_copy/gpt2_trained_model.cp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = pd.read_csv('data/pokemon_mapping.csv')\n",
    "\n",
    "model = [BertForSequenceClassification.from_pretrained(\"saved-model-base/\"),BertForSequenceClassification.from_pretrained(\"saved-model/\"),promptLoadedModel]\n",
    "tokenizer = [BertTokenizer.from_pretrained(\"saved-model-base/\"),BertTokenizer.from_pretrained(\"saved-model/\"),prompt_tokenizer]\n",
    "\n",
    "input_text = [\n",
    "    \"Walking stone monster with a huge body.\",\n",
    "    \"Walking stone monster with a huge body. It hates water.\",\n",
    "    \"Walking stone monster with a huge body. It hates water. Favorit attack is earthshake\",\n",
    "    \"Insect with sharp claws only found in the safari zone\",\n",
    "    \"only wakes up to eat\",\n",
    "    \"A rock pokemon which looks like a stone snake\",\n",
    "    \"A stone like snake\",\n",
    "    \"The pokemon has a small Flower on the head and likes to sing. During the night it is sleeping.\",\n",
    "    \"Many believe that all other Pok√©mon are descendants of this one\",\n",
    "    \"It was the result of various experiments of team rocket\",\n",
    "    \"A snake dragon like pokemon with a long tail. It is an higher evolution and is really strong. One of the top five is using this pokemon\",\n",
    "    \"It is yellow and it's cheeks have red circles. It has long ears and likes thunder. Ash is his best friend\",\n",
    "    \"A psychic pokemon with spoons\",\n",
    "    \"Red legendary dragon with fire\",\n",
    "]\n",
    "\n",
    "output = pretty_inference(model_list=model, tokenizer_list=tokenizer,model_names=[\"bert-base\",\"bert-large\", \"gpt2-prompting\"],\n",
    "input_list=input_text, top_n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "487b39d4bc77932302fbf00c8aa33c8cae154b5482e37c69cf95409c8a1ceaae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
