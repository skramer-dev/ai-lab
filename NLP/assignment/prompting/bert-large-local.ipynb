{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\transformers\\generation_utils.py:27: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader, PromptForClassification\n",
    "from openprompt.data_utils import InputExample, InputFeatures\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, ManualTemplate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dataset-294e9b13f49dafc6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/dataset to C:/Users/fst/.cache/huggingface/datasets/csv/dataset-294e9b13f49dafc6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b99731609f74b378850f28fe4a4fa9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cea90f31c0a44bbda2689c558f39d8ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb1b922f0d104fd5b0e931e145460a25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/fst/.cache/huggingface/datasets/csv/dataset-294e9b13f49dafc6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1362c0a7631a42b0bd11219b246e892b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "pokemon_descriptions = load_dataset('../data/dataset/', delimiter=';')\n",
    "NUM_CLASSES = np.unique(pokemon_descriptions['train']['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "split_pokemon_descriptions = pokemon_descriptions['train'].train_test_split(\n",
    "    test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate InputExamples from existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for split in ['train']:\n",
    "    dataset[split] = []\n",
    "    for sample in split_pokemon_descriptions[split]:\n",
    "        input_example = InputExample(text_a = sample['text'], label=int(sample['labels']))\n",
    "        dataset[split].append(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'text': 'Around 1 in 1000 people like to smell its drool, however', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' the pokemon is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'label': 43}]\n"
     ]
    }
   ],
   "source": [
    "promptTemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} the pokemon is {\"mask\"}',\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = pd.read_csv('../data/pokemon_mapping.csv')\n",
    "name_to_label_dict = mappings[[\"name\",\"index\"]].set_index('index').to_dict()[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = NUM_CLASSES,\n",
    "    label_words = name_to_label_dict,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 8631it [00:09, 935.54it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(\n",
    "  dataset=dataset[\"train\"],\n",
    "  template=promptTemplate, \n",
    "  tokenizer=tokenizer,\n",
    "  tokenizer_wrapper_class=WrapperClass, \n",
    "  shuffle=True,\n",
    "  truncate_method=\"head\",\n",
    "  decoder_max_length=3,\n",
    "  batch_size=2,\n",
    "  teacher_forcing=False,\n",
    "  predict_eos_token=False,\n",
    "  max_seq_length=327,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer,\n",
    "    freeze_plm= False\n",
    ")\n",
    "promptModel= promptModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "no_decay = ['bias', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in promptModel.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in promptModel.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 6.0881028175354\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        logits = promptModel(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step %100 == 0:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(promptModel.state_dict(),\"checkp/bert_large_trained_model.cp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versuch 1\n",
    "\n",
    "- True = [133, 12, 46, 44, 70, 0, 101, 134, 113, 109, 136, 38, 91, 143, 59, 110, 127, 116, 98, 80, 149, 48, 46]\n",
    "- Predicted = [ 94,  78,  94,  94,  78, 105,  94,  94,  94,  94,  54,  94,  94,  78, 98,  94,  78,  94,  94, 105,  28,  78,  94]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = {}\n",
    "for split in ['test']:\n",
    "    dataset_test[split] = []\n",
    "    for sample in split_pokemon_descriptions[split]:\n",
    "        input_example = InputExample(text_a = sample['text'], label=int(sample['labels']))\n",
    "        dataset_test[split].append(input_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 2158it [00:02, 1002.80it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = PromptDataLoader(dataset=dataset_test[\"test\"], template=promptTemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=250, decoder_max_length=3,\n",
    "    batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0074142724745134385\n"
     ]
    }
   ],
   "source": [
    "allpreds = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "    inputs = inputs.cuda()\n",
    "    logits = promptModel(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllabels.extend(labels.cuda().tolist())\n",
    "    allpreds.extend(torch.argmax(logits, dim=-1).cuda().tolist())\n",
    "\n",
    "acc = sum([int(i==j) for i,j in zip(allpreds, alllabels)])/len(allpreds)\n",
    "print(acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45037c9fa964e33746a67959e611b1255904e81b18931a74a47f598c93f55abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
