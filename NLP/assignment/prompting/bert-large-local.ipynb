{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt import PromptDataLoader, PromptForClassification\n",
    "from openprompt.data_utils import InputExample\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, ManualTemplate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 571/571 [00:00<00:00, 231kB/s]\n",
      "Downloading: 100%|██████████| 1.34G/1.34G [00:45<00:00, 29.8MB/s]\n",
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading: 100%|██████████| 232k/232k [00:00<00:00, 595kB/s] \n",
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 19.0kB/s]\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"bert\", \"bert-large-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(dataset_split, template, shuffle = False, batch_size=32):\n",
    "  \"\"\" Returns a prompt data load for a given dataset split and template \"\"\"\n",
    "\n",
    "  return PromptDataLoader(\n",
    "    dataset=dataset_split,\n",
    "    template=template, \n",
    "    tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, \n",
    "    shuffle=shuffle,\n",
    "    truncate_method=\"head\",\n",
    "    decoder_max_length=3,\n",
    "    batch_size=batch_size,\n",
    "    teacher_forcing=False,\n",
    "    predict_eos_token=False,\n",
    "    max_seq_length=64,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.prompts import ManualTemplate\n",
    "template = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} The pokemon is {\"mask\"}.',\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dataset-c155d0d0dfd2c295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/dataset to C:/Users/CYBORGX/.cache/huggingface/datasets/csv/dataset-c155d0d0dfd2c295/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1994.44it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 200.08it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to C:/Users/CYBORGX/.cache/huggingface/datasets/csv/dataset-c155d0d0dfd2c295/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 104.95it/s]\n",
      "Parameter 'generator'=Generator(PCG64) of the transform datasets.arrow_dataset.Dataset.train_test_split couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "pokemon_descriptions = load_dataset('../data/dataset/', delimiter=';')\n",
    "\n",
    "NUM_CLASSES = len(np.unique(pokemon_descriptions['train']['labels']))\n",
    "\n",
    "# train test split\n",
    "split_pokemon_descriptions = pokemon_descriptions['train'].train_test_split(\n",
    "    test_size=0.2, shuffle=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('birdclef')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a48a9edd0c746375c1d1284f7b88af601086e84b8f520d63fb12c80657ee0433"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
