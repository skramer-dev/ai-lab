{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\transformers\\generation_utils.py:27: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "from openprompt import PromptDataLoader, PromptForClassification\n",
    "from openprompt.data_utils import InputExample, InputFeatures\n",
    "from openprompt.plms import load_plm\n",
    "from openprompt.prompts import ManualTemplate, ManualVerbalizer, ManualTemplate\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "plm, tokenizer, model_config, WrapperClass = load_plm(\"gpt2\",\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics and Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  \"\"\" Computes the metrics given a tuple of (logits, labels) \"\"\"\n",
    "\n",
    "  load_accuracy = load_metric(\"accuracy\")\n",
    "  precision_metric = load_metric('precision')\n",
    "  recall_metric = load_metric('recall')\n",
    "  load_f1 = load_metric(\"f1\")\n",
    "   \n",
    "  logits, labels = eval_pred\n",
    "  predictions = np.argmax(logits, axis=-1)\n",
    "  accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "  precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\", zero_division=0)[\"precision\"]\n",
    "  recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\", zero_division=0)[\"recall\"]\n",
    "  f1 = load_f1.compute(predictions=predictions, references=labels, average=\"macro\")[\"f1\"]\n",
    "   \n",
    "  return {\n",
    "    \"accuracy\": accuracy,\n",
    "    \"f1\": f1,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "  }\n",
    "\n",
    "def evaluate(data_loader, prompt_model, desc=\"Validation\"):\n",
    "  prompt_model.eval()\n",
    "  all_logits = []\n",
    "  all_labels = []\n",
    "  with torch.no_grad():\n",
    "    for inputs in tqdm(data_loader, desc=\"Validation\"):\n",
    "        inputs = inputs.cuda()\n",
    "        logits = prompt_model(inputs)\n",
    "        all_logits.extend(logits.cpu().tolist())\n",
    "        all_labels.extend(inputs['label'].cpu().tolist())\n",
    "  return (all_logits, all_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration dataset-294e9b13f49dafc6\n",
      "Found cached dataset csv (C:/Users/fst/.cache/huggingface/datasets/csv/dataset-294e9b13f49dafc6/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386eb66893e14ba6b5f1042a8970d9d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "pokemon_descriptions = load_dataset('../data/dataset/', delimiter=';')\n",
    "NUM_CLASSES = np.unique(pokemon_descriptions['train']['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_pokemon_descriptions = pokemon_descriptions['train'].train_test_split(\n",
    "    test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate InputExamples from existing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "for split in ['train','test']:\n",
    "    dataset[split] = []\n",
    "    for sample in split_pokemon_descriptions[split]:\n",
    "        input_example = InputExample(text_a = sample['text'], label=int(sample['labels']))\n",
    "        dataset[split].append(input_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptTemplate = ManualTemplate(\n",
    "    text = '{\"placeholder\":\"text_a\"} the pokemon is {\"mask\"}',\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create verbalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = pd.read_csv('../data/pokemon_mapping.csv')\n",
    "name_to_label_dict = mappings[[\"name\",\"index\"]].set_index('index').to_dict()[\"name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptVerbalizer = ManualVerbalizer(\n",
    "    classes = NUM_CLASSES,\n",
    "    label_words = name_to_label_dict,\n",
    "    tokenizer = tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing: 8631it [00:06, 1378.67it/s]\n",
      "tokenizing: 2158it [00:01, 1672.79it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = PromptDataLoader(\n",
    "  dataset=dataset[\"train\"],\n",
    "  template=promptTemplate, \n",
    "  tokenizer=tokenizer,\n",
    "  tokenizer_wrapper_class=WrapperClass, \n",
    "  shuffle=True,\n",
    "  truncate_method=\"head\",\n",
    "  decoder_max_length=3,\n",
    "  batch_size=2,\n",
    "  teacher_forcing=False,\n",
    "  predict_eos_token=False,\n",
    "  max_seq_length=327,\n",
    ")\n",
    "\n",
    "test_dataloader = PromptDataLoader(dataset=dataset[\"test\"], template=promptTemplate, tokenizer=tokenizer,\n",
    "    tokenizer_wrapper_class=WrapperClass, max_seq_length=128, decoder_max_length=3,\n",
    "    batch_size=1,shuffle=False, teacher_forcing=False, predict_eos_token=False,\n",
    "    truncate_method=\"head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "promptModel = PromptForClassification(\n",
    "    template = promptTemplate,\n",
    "    plm = plm,\n",
    "    verbalizer = promptVerbalizer,\n",
    "    freeze_plm= False\n",
    ")\n",
    "promptModel=promptModel.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "no_decay = ['bias', 'layer_norm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in promptModel.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in promptModel.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]\n",
    "optimizer = torch.optim.AdamW(params=optimizer_grouped_parameters, lr=3e-5)\n",
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss: 5.516399383544922\n",
      "Epoch 0, average loss: 3.977995147940931\n",
      "Epoch 0, average loss: 3.9102446995180506\n",
      "Epoch 0, average loss: 3.798496606729268\n",
      "Epoch 0, average loss: 3.698083406331687\n",
      "Epoch 0, average loss: 3.6065098579229535\n",
      "Epoch 0, average loss: 3.511069106723816\n",
      "Epoch 0, average loss: 3.459858936424834\n",
      "Epoch 0, average loss: 3.402552617653251\n",
      "Epoch 0, average loss: 3.3727770181114294\n",
      "Epoch 0, average loss: 3.3256734763150146\n",
      "Epoch 0, average loss: 3.301160977366628\n",
      "Epoch 0, average loss: 3.268777855792284\n",
      "Epoch 0, average loss: 3.247982732464245\n",
      "Epoch 0, average loss: 3.2225674094846233\n",
      "Epoch 0, average loss: 3.1680162519802675\n",
      "Epoch 0, average loss: 3.1380975018421116\n",
      "Epoch 0, average loss: 3.1085160775570926\n",
      "Epoch 0, average loss: 3.0629945724450947\n",
      "Epoch 0, average loss: 3.036026598298383\n",
      "Epoch 0, average loss: 3.0069157800480215\n",
      "Epoch 0, average loss: 2.979515702590631\n",
      "Epoch 0, average loss: 2.958554797322269\n",
      "Epoch 0, average loss: 2.9334431724123315\n",
      "Epoch 0, average loss: 2.898183446474833\n",
      "Epoch 0, average loss: 2.8773094402900963\n",
      "Epoch 0, average loss: 2.8519526958345893\n",
      "Epoch 0, average loss: 2.8299621422843986\n",
      "Epoch 0, average loss: 2.8006637306763236\n",
      "Epoch 1, average loss: 0.6525099277496338\n",
      "Epoch 1, average loss: 1.3375187527991805\n",
      "Epoch 1, average loss: 1.3137215129065702\n",
      "Epoch 1, average loss: 1.377366593611175\n",
      "Epoch 1, average loss: 1.3798715692136436\n",
      "Epoch 1, average loss: 1.3641171536842611\n",
      "Epoch 1, average loss: 1.3593326640116452\n",
      "Epoch 1, average loss: 1.3820562285811993\n",
      "Epoch 1, average loss: 1.3843836449484836\n",
      "Epoch 1, average loss: 1.3910356636573618\n",
      "Epoch 1, average loss: 1.3811820973535234\n",
      "Epoch 1, average loss: 1.38852420269706\n",
      "Epoch 1, average loss: 1.4012553549434212\n",
      "Epoch 1, average loss: 1.404774460393115\n",
      "Epoch 1, average loss: 1.406892046231604\n",
      "Epoch 1, average loss: 1.4064340290089872\n",
      "Epoch 1, average loss: 1.4139991759421444\n",
      "Epoch 1, average loss: 1.4227244499605636\n",
      "Epoch 1, average loss: 1.429632922034246\n",
      "Epoch 1, average loss: 1.4328168744666234\n",
      "Epoch 1, average loss: 1.4268582261754668\n",
      "Epoch 1, average loss: 1.4274436554692864\n",
      "Epoch 1, average loss: 1.4252001537476526\n",
      "Epoch 1, average loss: 1.4223518184307087\n",
      "Epoch 1, average loss: 1.4180867519287\n",
      "Epoch 1, average loss: 1.4171769222192998\n",
      "Epoch 1, average loss: 1.4169259911895242\n",
      "Epoch 1, average loss: 1.4090427357156108\n",
      "Epoch 1, average loss: 1.408336972734216\n",
      "Epoch 2, average loss: 2.967986822128296\n",
      "Epoch 2, average loss: 0.7232906330983099\n",
      "Epoch 2, average loss: 0.6517132161319497\n",
      "Epoch 2, average loss: 0.6686785243569818\n",
      "Epoch 2, average loss: 0.6666925085267887\n",
      "Epoch 2, average loss: 0.6781391470421827\n",
      "Epoch 2, average loss: 0.671522503595333\n",
      "Epoch 2, average loss: 0.6630823537433995\n",
      "Epoch 2, average loss: 0.6681386275850726\n",
      "Epoch 2, average loss: 0.6814645747453988\n",
      "Epoch 2, average loss: 0.6848341371354837\n",
      "Epoch 2, average loss: 0.6891969336411877\n",
      "Epoch 2, average loss: 0.6897953289196085\n",
      "Epoch 2, average loss: 0.6954490342667432\n",
      "Epoch 2, average loss: 0.6997699688367448\n",
      "Epoch 2, average loss: 0.7031043576289208\n",
      "Epoch 2, average loss: 0.7074156127274894\n",
      "Epoch 2, average loss: 0.7143447925999663\n",
      "Epoch 2, average loss: 0.7214793211481082\n",
      "Epoch 2, average loss: 0.7192932199966853\n",
      "Epoch 2, average loss: 0.7224769779472364\n",
      "Epoch 2, average loss: 0.7256494945390632\n",
      "Epoch 2, average loss: 0.7301134310394224\n",
      "Epoch 2, average loss: 0.7300262428059147\n",
      "Epoch 2, average loss: 0.7314740469460612\n",
      "Epoch 2, average loss: 0.7335742806848078\n",
      "Epoch 2, average loss: 0.7369586944202605\n",
      "Epoch 2, average loss: 0.7399451685174648\n",
      "Epoch 2, average loss: 0.7457154738067968\n",
      "Epoch 3, average loss: 0.3673666715621948\n",
      "Epoch 3, average loss: 0.5206519342557728\n",
      "Epoch 3, average loss: 0.5372096812930122\n",
      "Epoch 3, average loss: 0.5282275297824448\n",
      "Epoch 3, average loss: 0.5150575789074967\n",
      "Epoch 3, average loss: 0.5138894022834615\n",
      "Epoch 3, average loss: 0.5231647107137145\n",
      "Epoch 3, average loss: 0.5309797970273636\n",
      "Epoch 3, average loss: 0.5329201320198407\n",
      "Epoch 3, average loss: 0.5342511010577106\n",
      "Epoch 3, average loss: 0.5312863026823732\n",
      "Epoch 3, average loss: 0.5374519877638885\n",
      "Epoch 3, average loss: 0.5349732032772287\n",
      "Epoch 3, average loss: 0.5456388761436296\n",
      "Epoch 3, average loss: 0.5466792527374023\n",
      "Epoch 3, average loss: 0.5547716608522807\n",
      "Epoch 3, average loss: 0.5548183626505188\n",
      "Epoch 3, average loss: 0.5558804716279356\n",
      "Epoch 3, average loss: 0.556296302916115\n",
      "Epoch 3, average loss: 0.5555406327533664\n",
      "Epoch 3, average loss: 0.5541366303084925\n",
      "Epoch 3, average loss: 0.5538055825946006\n",
      "Epoch 3, average loss: 0.5576431590327372\n",
      "Epoch 3, average loss: 0.5611569688006023\n",
      "Epoch 3, average loss: 0.5600131047422701\n",
      "Epoch 3, average loss: 0.5609985477513083\n",
      "Epoch 3, average loss: 0.5601127018706932\n",
      "Epoch 3, average loss: 0.559988145105993\n",
      "Epoch 3, average loss: 0.5645909785427684\n",
      "Epoch 4, average loss: 0.0014206175692379475\n",
      "Epoch 4, average loss: 0.3903488592290254\n",
      "Epoch 4, average loss: 0.4458298548894869\n",
      "Epoch 4, average loss: 0.43892039365756613\n",
      "Epoch 4, average loss: 0.4407180151157363\n",
      "Epoch 4, average loss: 0.4345187833695762\n",
      "Epoch 4, average loss: 0.43274167343383224\n",
      "Epoch 4, average loss: 0.4465753175032224\n",
      "Epoch 4, average loss: 0.4448540289269386\n",
      "Epoch 4, average loss: 0.4520201534313715\n",
      "Epoch 4, average loss: 0.46535982319060054\n",
      "Epoch 4, average loss: 0.4651580826453998\n",
      "Epoch 4, average loss: 0.46369795946953823\n",
      "Epoch 4, average loss: 0.4662807342362317\n",
      "Epoch 4, average loss: 0.4683647938266532\n",
      "Epoch 4, average loss: 0.47718388664762484\n",
      "Epoch 4, average loss: 0.47710814776643734\n",
      "Epoch 4, average loss: 0.480252674684299\n",
      "Epoch 4, average loss: 0.48206044410735416\n",
      "Epoch 4, average loss: 0.48069928146571445\n",
      "Epoch 4, average loss: 0.48145017980571264\n",
      "Epoch 4, average loss: 0.48735489098316476\n",
      "Epoch 4, average loss: 0.49359801012737553\n",
      "Epoch 4, average loss: 0.4920102431442916\n",
      "Epoch 4, average loss: 0.49014905236534556\n",
      "Epoch 4, average loss: 0.4912005294372889\n",
      "Epoch 4, average loss: 0.4914312159985386\n",
      "Epoch 4, average loss: 0.49181185828653595\n",
      "Epoch 4, average loss: 0.49317460957704856\n",
      "Epoch 5, average loss: 0.34658950567245483\n",
      "Epoch 5, average loss: 0.513470881109618\n",
      "Epoch 5, average loss: 0.4725274379254822\n",
      "Epoch 5, average loss: 0.4490622498192515\n",
      "Epoch 5, average loss: 0.46761681173496944\n",
      "Epoch 5, average loss: 0.4758022067102816\n",
      "Epoch 5, average loss: 0.4724159463034518\n",
      "Epoch 5, average loss: 0.4745751526172636\n",
      "Epoch 5, average loss: 0.4791988504652343\n",
      "Epoch 5, average loss: 0.47844007454415477\n",
      "Epoch 5, average loss: 0.473490325076725\n",
      "Epoch 5, average loss: 0.4774476975431327\n",
      "Epoch 5, average loss: 0.4744024949740083\n",
      "Epoch 5, average loss: 0.486085860697947\n",
      "Epoch 5, average loss: 0.4850940665172001\n",
      "Epoch 5, average loss: 0.4810075595033032\n",
      "Epoch 5, average loss: 0.4813327514059046\n",
      "Epoch 5, average loss: 0.4897980823409077\n",
      "Epoch 5, average loss: 0.4896140971031038\n",
      "Epoch 5, average loss: 0.49041887290567127\n",
      "Epoch 5, average loss: 0.4909858465767013\n",
      "Epoch 5, average loss: 0.49323902282482357\n",
      "Epoch 5, average loss: 0.4901860552026202\n",
      "Epoch 5, average loss: 0.49606993014430334\n",
      "Epoch 5, average loss: 0.49788931427063055\n",
      "Epoch 5, average loss: 0.4950893134270121\n",
      "Epoch 5, average loss: 0.49515209939906896\n",
      "Epoch 5, average loss: 0.4948764668271235\n",
      "Epoch 5, average loss: 0.48845659803672914\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    tot_loss = 0\n",
    "    for step, inputs in enumerate(train_dataloader):\n",
    "        inputs = inputs.cuda()\n",
    "        logits = promptModel(inputs)\n",
    "        labels = inputs['label']\n",
    "        loss = loss_func(logits, labels)\n",
    "        loss.backward()\n",
    "        tot_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if step % 150 == 0:\n",
    "            print(\"Epoch {}, average loss: {}\".format(epoch, tot_loss/(step+1)), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(promptModel.state_dict(),\"checkp/gpt2_trained_model.cp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\python\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5301204819277109,\n",
       " 'f1': 0.46870223230762326,\n",
       " 'precision': 0.4638712342506931,\n",
       " 'recall': 0.5187880442264253}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alllogits = []\n",
    "alllabels = []\n",
    "for step, inputs in enumerate(test_dataloader):\n",
    "    inputs = inputs.cuda()\n",
    "    logits = promptModel(inputs)\n",
    "    labels = inputs['label']\n",
    "    alllogits.extend(logits.cuda().tolist())\n",
    "    alllabels.extend(inputs['label'].cuda().tolist())\n",
    "\n",
    "compute_metrics((alllogits,alllabels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "45037c9fa964e33746a67959e611b1255904e81b18931a74a47f598c93f55abd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
