{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def resize_frame(frame):\n",
    "    frame = frame[30:-12,5:-4]\n",
    "    frame = np.average(frame,axis = 2)\n",
    "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
    "    frame = np.array(frame,dtype = np.uint8)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.frames = deque(maxlen = max_len)\n",
    "        self.actions = deque(maxlen = max_len)\n",
    "        self.rewards = deque(maxlen = max_len)\n",
    "        self.done_flags = deque(maxlen = max_len)\n",
    "\n",
    "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
    "        self.frames.append(next_frame)\n",
    "        self.actions.append(next_action)\n",
    "        self.rewards.append(next_frames_reward)\n",
    "        self.done_flags.append(next_frame_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def initialize_new_game(name, env, agent):\n",
    "    \"\"\"We don't want an agents past game influencing its new game, so we add in some dummy data to initialize\"\"\"\n",
    "    \n",
    "    env.reset()\n",
    "    starting_frame = resize_frame(env.step(0)[0])\n",
    "\n",
    "    dummy_action = 0\n",
    "    dummy_reward = 0\n",
    "    dummy_done = False\n",
    "    for i in range(3):\n",
    "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
    "\n",
    "def make_env(name, agent):\n",
    "    env = gym.make(name, render_mode='human')\n",
    "    return env\n",
    "\n",
    "def take_step(name, env, agent, score, debug):\n",
    "    \n",
    "    #1 and 2: Update timesteps and save weights\n",
    "    agent.total_timesteps += 1\n",
    "    if agent.total_timesteps % 50000 == 0:\n",
    "      agent.model.save_weights('recent_weights.hdf5')\n",
    "      print('\\nWeights saved!')\n",
    "\n",
    "    #3: Take action\n",
    "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(agent.memory.actions[-1])\n",
    "    \n",
    "    #4: Get next state\n",
    "    next_frame = resize_frame(next_frame)\n",
    "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
    "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
    "    new_state = np.expand_dims(new_state,0) #^^^\n",
    "    \n",
    "    #5: Get next action, using next state\n",
    "    next_action = agent.get_action(new_state)\n",
    "\n",
    "    #6: If game is over, return the score\n",
    "    if next_frame_terminal:\n",
    "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "        return (score + next_frames_reward),True\n",
    "\n",
    "    #7: Now we add the next experience to memory\n",
    "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "\n",
    "    #8: If we are trying to debug this then render\n",
    "    # if debug:\n",
    "    #     # env.render()\n",
    "\n",
    "    #9: If the threshold memory is satisfied, make the agent learn from memory\n",
    "    if len(agent.memory.frames) > agent.starting_mem_len:\n",
    "        agent.learn(debug)\n",
    "\n",
    "    return (score + next_frames_reward),False\n",
    "\n",
    "def play_episode(name, env, agent, debug = False):\n",
    "    initialize_new_game(name, env, agent)\n",
    "    done = False\n",
    "    score = 0\n",
    "    while True:\n",
    "        score,done = take_step(name,env,agent,score, debug)\n",
    "        if done:\n",
    "            break\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "class Agent():\n",
    "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.possible_actions = possible_actions\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = .9/100000\n",
    "        self.epsilon_min = .05\n",
    "        self.gamma = .95\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.lives = starting_lives #this parameter does not apply to pong\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input((84,84,4)))\n",
    "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
    "        model.summary()\n",
    "        print('\\nAgent Initialized\\n')\n",
    "        return model\n",
    "\n",
    "    def get_action(self,state):\n",
    "        \"\"\"Explore\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.sample(self.possible_actions,1)[0]\n",
    "\n",
    "        \"\"\"Do Best Acton\"\"\"\n",
    "        a_index = np.argmax(self.model.predict(state))\n",
    "        return self.possible_actions[a_index]\n",
    "\n",
    "    def _index_valid(self,index):\n",
    "        if self.memory.done_flags[index-3] or self.memory.done_flags[index-2] or self.memory.done_flags[index-1] or self.memory.done_flags[index]:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def learn(self,debug = False):\n",
    "        \"\"\"we want the output[a] to be R_(t+1) + Qmax_(t+1).\"\"\"\n",
    "        \"\"\"So target for taking action 1 should be [output[0], R_(t+1) + Qmax_(t+1), output[2]]\"\"\"\n",
    "\n",
    "        \"\"\"First we need 32 random valid indicies\"\"\"\n",
    "        states = []\n",
    "        next_states = []\n",
    "        actions_taken = []\n",
    "        next_rewards = []\n",
    "        next_done_flags = []\n",
    "\n",
    "        while len(states) < 32:\n",
    "            index = np.random.randint(4,len(self.memory.frames) - 1)\n",
    "            if self._index_valid(index):\n",
    "                state = [self.memory.frames[index-3], self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index]]\n",
    "                state = np.moveaxis(state,0,2)/255\n",
    "                next_state = [self.memory.frames[index-2], self.memory.frames[index-1], self.memory.frames[index], self.memory.frames[index+1]]\n",
    "                next_state = np.moveaxis(next_state,0,2)/255\n",
    "\n",
    "                states.append(state)\n",
    "                next_states.append(next_state)\n",
    "                actions_taken.append(self.memory.actions[index])\n",
    "                next_rewards.append(self.memory.rewards[index+1])\n",
    "                next_done_flags.append(self.memory.done_flags[index+1])\n",
    "\n",
    "        \"\"\"Now we get the ouputs from our model, and the target model. We need this for our target in the error function\"\"\"\n",
    "        labels = self.model.predict(np.array(states))\n",
    "        next_state_values = self.model_target.predict(np.array(next_states))\n",
    "        \n",
    "        \"\"\"Now we define our labels, or what the output should have been\n",
    "           We want the output[action_taken] to be R_(t+1) + Qmax_(t+1) \"\"\"\n",
    "        for i in range(32):\n",
    "            action = self.possible_actions.index(actions_taken[i])\n",
    "            labels[i][action] = next_rewards[i] + (not next_done_flags[i]) * self.gamma * max(next_state_values[i])\n",
    "\n",
    "        \"\"\"Train our model using the states and outputs generated\"\"\"\n",
    "        self.model.fit(np.array(states),labels,batch_size = 32, epochs = 1, verbose = 0)\n",
    "\n",
    "        \"\"\"Decrease epsilon and update how many times our agent has learned\"\"\"\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon -= self.epsilon_decay\n",
    "        self.learns += 1\n",
    "        \n",
    "        \"\"\"Every 10000 learned, copy our model weights to our target model\"\"\"\n",
    "        if self.learns % 10000 == 0:\n",
    "            self.model_target.set_weights(self.model.get_weights())\n",
    "            print('\\nTarget model updated')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [04:00<1:17:58, 49.24s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#name = 'PongDeterministic-v4'\n",
    "name = 'Atlantis-v0'\n",
    "\n",
    "agent = Agent(possible_actions=[0,1,2,3],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .00025)\n",
    "env = make_env(name,agent)\n",
    "\n",
    "last_100_avg = [-21]\n",
    "scores = deque(maxlen = 100)\n",
    "max_score = -21\n",
    "\n",
    "\"\"\" If testing:\n",
    "agent.model.load_weights('recent_weights.hdf5')\n",
    "agent.model_target.load_weights('recent_weights.hdf5')\n",
    "agent.epsilon = 0.0\n",
    "\"\"\"\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    timesteps = agent.total_timesteps\n",
    "    timee = time.time()\n",
    "    score = play_episode(name, env, agent, debug = True) #set debug to true for rendering\n",
    "    scores.append(score)\n",
    "    if score > max_score:\n",
    "        max_score = score\n",
    "\n",
    "    print('\\nEpisode: ' + str(i))\n",
    "    print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
    "    print('Duration: ' + str(time.time() - timee))\n",
    "    print('Score: ' + str(score))\n",
    "    print('Max Score: ' + str(max_score))\n",
    "    print('Epsilon: ' + str(agent.epsilon))\n",
    "    clear_output()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki-tf-3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 24 2022, 14:07:00) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdcbfc4cbef837ff2e25f30cc98e845d74e97ba65f0c18a49caf8ac07d18543f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
