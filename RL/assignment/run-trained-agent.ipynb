{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers.monitoring import video_recorder\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential, clone_model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import time\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_frame(frame):\n",
    "    frame = frame[30:-12,5:-4]\n",
    "    frame = np.average(frame,axis = 2)\n",
    "    frame = cv2.resize(frame,(84,84),interpolation = cv2.INTER_NEAREST)\n",
    "    frame = np.array(frame,dtype = np.uint8)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self,max_len):\n",
    "        self.max_len = max_len\n",
    "        self.frames = deque(maxlen = max_len)\n",
    "        self.actions = deque(maxlen = max_len)\n",
    "        self.rewards = deque(maxlen = max_len)\n",
    "        self.done_flags = deque(maxlen = max_len)\n",
    "\n",
    "    def add_experience(self,next_frame, next_frames_reward, next_action, next_frame_terminal):\n",
    "        self.frames.append(next_frame)\n",
    "        self.actions.append(next_action)\n",
    "        self.rewards.append(next_frames_reward)\n",
    "        self.done_flags.append(next_frame_terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_new_game(name, env, agent):\n",
    "    env.reset()\n",
    "    starting_frame = resize_frame(env.step(0)[0])\n",
    "\n",
    "    dummy_action = 0\n",
    "    dummy_reward = 0\n",
    "    dummy_done = False\n",
    "    for i in range(3):\n",
    "        agent.memory.add_experience(starting_frame, dummy_reward, dummy_action, dummy_done)\n",
    "\n",
    "def make_env(name, agent):\n",
    "    env = gym.make(name, render_mode='human')\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_step(name, env, agent, score, debug):\n",
    "    \n",
    "    #1 and 2: Update timesteps and save weights\n",
    "    agent.total_timesteps += 1\n",
    "\n",
    "    #3: Take action\n",
    "    next_frame, next_frames_reward, next_frame_terminal, info = env.step(agent.memory.actions[-1])\n",
    "    \n",
    "    #4: Get next state\n",
    "    next_frame = resize_frame(next_frame)\n",
    "    new_state = [agent.memory.frames[-3], agent.memory.frames[-2], agent.memory.frames[-1], next_frame]\n",
    "    new_state = np.moveaxis(new_state,0,2)/255 #We have to do this to get it into keras's goofy format of [batch_size,rows,columns,channels]\n",
    "    new_state = np.expand_dims(new_state,0) #^^^\n",
    "    \n",
    "    #5: Get next action, using next state\n",
    "    next_action = agent.get_action(new_state)\n",
    "\n",
    "    #6: If game is over, return the score\n",
    "    if next_frame_terminal:\n",
    "        agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "        return (score + next_frames_reward),True\n",
    "\n",
    "    #7: Now we add the next experience to memory\n",
    "    agent.memory.add_experience(next_frame, next_frames_reward, next_action, next_frame_terminal)\n",
    "\n",
    "    return (score + next_frames_reward),False\n",
    "\n",
    "def play_episode(name, env, agent, debug = False):\n",
    "    initialize_new_game(name, env, agent)\n",
    "\n",
    "    # time delay to use external recording software like OBS to capture the gameplay\n",
    "    time.sleep(1)\n",
    "\n",
    "    done = False\n",
    "    score = 0\n",
    "    while True:\n",
    "        score,done = take_step(name,env,agent,score, debug)\n",
    "        if done:\n",
    "            break\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self,possible_actions,starting_mem_len,max_mem_len,starting_epsilon,learn_rate, starting_lives = 5, debug = False):\n",
    "        self.memory = Memory(max_mem_len)\n",
    "        self.possible_actions = possible_actions\n",
    "        self.epsilon = starting_epsilon\n",
    "        self.epsilon_decay = .9/100000\n",
    "        self.epsilon_min = .05\n",
    "        self.gamma = .95\n",
    "        self.learn_rate = learn_rate\n",
    "        self.model = self._build_model()\n",
    "        self.model_target = clone_model(self.model)\n",
    "        self.total_timesteps = 0\n",
    "        self.lives = starting_lives #this parameter does not apply to pong\n",
    "        self.starting_mem_len = starting_mem_len\n",
    "        self.learns = 0\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input((84,84,4)))\n",
    "        model.add(Conv2D(filters = 32,kernel_size = (8,8),strides = 4,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (4,4),strides = 2,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Conv2D(filters = 64,kernel_size = (3,3),strides = 1,data_format=\"channels_last\", activation = 'relu',kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation = 'relu', kernel_initializer = tf.keras.initializers.VarianceScaling(scale=2)))\n",
    "        model.add(Dense(len(self.possible_actions), activation = 'linear'))\n",
    "        optimizer = Adam(self.learn_rate)\n",
    "        model.compile(optimizer, loss=tf.keras.losses.Huber())\n",
    "        print('\\nAgent Initialized\\n')\n",
    "        return model\n",
    "\n",
    "    def get_action(self,state):\n",
    "        \"\"\"Explore\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.sample(self.possible_actions,1)[0]\n",
    "\n",
    "        \"\"\"Do Best Acton\"\"\"\n",
    "        a_index = np.argmax(self.model.predict(state))\n",
    "        return self.possible_actions[a_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Agent Initialized\n",
      "\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m timesteps \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtotal_timesteps\n\u001b[0;32m     19\u001b[0m timee \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 20\u001b[0m score \u001b[39m=\u001b[39m play_episode(name, env, agent, debug \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m) \u001b[39m#set debug to true for rendering\u001b[39;00m\n\u001b[0;32m     21\u001b[0m scores\u001b[39m.\u001b[39mappend(score)\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m score \u001b[39m>\u001b[39m max_score:\n",
      "Cell \u001b[1;32mIn[5], line 37\u001b[0m, in \u001b[0;36mplay_episode\u001b[1;34m(name, env, agent, debug)\u001b[0m\n\u001b[0;32m     35\u001b[0m score \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m     score,done \u001b[39m=\u001b[39m take_step(name,env,agent,score, debug)\n\u001b[0;32m     38\u001b[0m     \u001b[39mif\u001b[39;00m done:\n\u001b[0;32m     39\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mtake_step\u001b[1;34m(name, env, agent, score, debug)\u001b[0m\n\u001b[0;32m      4\u001b[0m agent\u001b[39m.\u001b[39mtotal_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[39m#3: Take action\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m next_frame, next_frames_reward, next_frame_terminal, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(agent\u001b[39m.\u001b[39;49mmemory\u001b[39m.\u001b[39;49mactions[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[0;32m      9\u001b[0m \u001b[39m#4: Get next state\u001b[39;00m\n\u001b[0;32m     10\u001b[0m next_frame \u001b[39m=\u001b[39m resize_frame(next_frame)\n",
      "File \u001b[1;32mc:\\Users\\Sebastian\\anaconda3\\envs\\ki-tf-3_10\\lib\\site-packages\\gym\\wrappers\\time_limit.py:18\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[0;32m     15\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     ), \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 18\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     19\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     20\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_elapsed_steps \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[1;32mc:\\Users\\Sebastian\\anaconda3\\envs\\ki-tf-3_10\\lib\\site-packages\\gym\\envs\\atari\\environment.py:238\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[1;34m(self, action_ind)\u001b[0m\n\u001b[0;32m    236\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m    237\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(frameskip):\n\u001b[1;32m--> 238\u001b[0m     reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39male\u001b[39m.\u001b[39mact(action)\n\u001b[0;32m    240\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_obs(), reward, terminal, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_info()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "name = 'Atlantis-v0'\n",
    "\n",
    "agent = Agent(possible_actions=[0,1,2,3],starting_mem_len=50000,max_mem_len=750000,starting_epsilon = 1, learn_rate = .00025)\n",
    "env = make_env(name,agent)\n",
    "\n",
    "last_100_avg = [-21]\n",
    "scores = deque(maxlen = 100)\n",
    "max_score = -21\n",
    "\n",
    "agent.model.load_weights('recent_weights.hdf5')\n",
    "agent.model_target.load_weights('recent_weights.hdf5')\n",
    "agent.epsilon = 0.0\n",
    "\n",
    "\n",
    "env.reset()\n",
    "\n",
    "\n",
    "timesteps = agent.total_timesteps\n",
    "timee = time.time()\n",
    "score = play_episode(name, env, agent, debug = True) #set debug to true for rendering\n",
    "scores.append(score)\n",
    "if score > max_score:\n",
    "    max_score = score\n",
    "\n",
    "print('Steps: ' + str(agent.total_timesteps - timesteps))\n",
    "print('Duration: ' + str(time.time() - timee))\n",
    "print('Score: ' + str(score))\n",
    "print('Max Score: ' + str(max_score))\n",
    "print('Epsilon: ' + str(agent.epsilon))\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ki-tf-3_10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cdcbfc4cbef837ff2e25f30cc98e845d74e97ba65f0c18a49caf8ac07d18543f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
